{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.2.1\n",
      "  Downloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn==1.2.1) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn==1.2.1) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn==1.2.1) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn==1.2.1) (3.3.0)\n",
      "Downloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.4.1.post1\n",
      "    Uninstalling scikit-learn-1.4.1.post1:\n",
      "      Successfully uninstalled scikit-learn-1.4.1.post1\n",
      "Successfully installed scikit-learn-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==1.2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pefile\n",
      "  Downloading pefile-2023.2.7-py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading pefile-2023.2.7-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m937.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pefile\n",
      "Successfully installed pefile-2023.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (4.66.2)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.12.25\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we build on the previous exercises to prepare a labeled dataset of binary feature vectors, and use it to train a *Random Forest* binary classifier of malware/benign feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directoriesWithLabels = [(\"Training a Static Malware Detector/Code/Samples/Benign\",0), (\"Training a Static Malware Detector/Code/Samples/Malware\",1)]\n",
    "listOfSamples = []\n",
    "labels = []\n",
    "for datasetPath, label in directoriesWithLabels:\n",
    "    samples = [f for f in os.listdir(datasetPath)]\n",
    "    for file in samples:\n",
    "        filePath = os.path.join(datasetPath, file)\n",
    "        listOfSamples.append(filePath)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-Test data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "samples_train, samples_test, labels_train, labels_test = train_test_split(listOfSamples, labels, test_size=0.33, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "import pefile\n",
    "\n",
    "def readFile(filePath):\n",
    "    with open(filePath, \"rb\") as binary_file:\n",
    "        data = binary_file.read()\n",
    "    return data\n",
    "\n",
    "def byteSequenceToNgrams(byteSequence, n):\n",
    "    Ngrams = ngrams(byteSequence, n)\n",
    "    return list(Ngrams)\n",
    "    \n",
    "def extractNgramCounts(file, N):\n",
    "    fileByteSequence = readFile(file)\n",
    "    fileNgrams = byteSequenceToNgrams(fileByteSequence, N)\n",
    "    return collections.Counter(fileNgrams)\n",
    "\n",
    "def getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list):\n",
    "    K1 = len(K1_most_common_Ngrams_list)\n",
    "    fv = K1*[0]\n",
    "    fileNgrams = extractNgramCounts(file, N)\n",
    "    for i in range(K1):\n",
    "        fv[i]=fileNgrams[K1_most_common_Ngrams_list[i]]\n",
    "    return fv\n",
    "\n",
    "def preprocessImports(listOfDLLs):\n",
    "    processedListOfDLLs = []\n",
    "    temp = [x.decode().split(\".\")[0].lower() for x in listOfDLLs]\n",
    "    return \" \".join(temp)\n",
    "\n",
    "def getImports(pe):\n",
    "    listOfImports = []\n",
    "    for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
    "        listOfImports.append(entry.dll)\n",
    "    return preprocessImports(listOfImports)\n",
    "\n",
    "def getSectionNames(pe):\n",
    "    listOfSectionNames = []\n",
    "    for eachSection in pe.sections:\n",
    "        refined_name = eachSection.Name.decode().replace('\\x00','').lower()\n",
    "        listOfSectionNames.append(refined_name)\n",
    "    return \" \".join(listOfSectionNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2-Grams, \n",
    "# and produce feature vectors based on the frequency method\n",
    "# This may take a few minutes to run\n",
    "N=2\n",
    "totalNgramCount = collections.Counter([])\n",
    "for file in samples_train:\n",
    "    totalNgramCount += extractNgramCounts(file, N)\n",
    "K1 = 100\n",
    "K1_most_common_Ngrams = totalNgramCount.most_common(K1)\n",
    "K1_most_common_Ngrams_list = [x[0] for x in K1_most_common_Ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (255, 255),\n",
       " (204, 204),\n",
       " (2, 100),\n",
       " (1, 0),\n",
       " (0, 139),\n",
       " (131, 196),\n",
       " (2, 0),\n",
       " (68, 36),\n",
       " (139, 69),\n",
       " (0, 131),\n",
       " (255, 117),\n",
       " (133, 192),\n",
       " (255, 139),\n",
       " (46, 46),\n",
       " (254, 255),\n",
       " (141, 77),\n",
       " (139, 77),\n",
       " (255, 21),\n",
       " (69, 252),\n",
       " (7, 0),\n",
       " (0, 1),\n",
       " (76, 36),\n",
       " (8, 139),\n",
       " (4, 0),\n",
       " (137, 69),\n",
       " (4, 139),\n",
       " (141, 69),\n",
       " (255, 131),\n",
       " (0, 137),\n",
       " (51, 192),\n",
       " (0, 255),\n",
       " (80, 232),\n",
       " (255, 141),\n",
       " (85, 139),\n",
       " (8, 0),\n",
       " (0, 116),\n",
       " (15, 182),\n",
       " (0, 232),\n",
       " (80, 141),\n",
       " (3, 100),\n",
       " (139, 236),\n",
       " (100, 0),\n",
       " (15, 132),\n",
       " (12, 139),\n",
       " (255, 0),\n",
       " (65, 68),\n",
       " (73, 78),\n",
       " (84, 36),\n",
       " (80, 65),\n",
       " (68, 68),\n",
       " (78, 71),\n",
       " (253, 255),\n",
       " (68, 73),\n",
       " (64, 0),\n",
       " (0, 204),\n",
       " (16, 0),\n",
       " (198, 69),\n",
       " (199, 69),\n",
       " (192, 116),\n",
       " (2, 101),\n",
       " (80, 255),\n",
       " (101, 0),\n",
       " (4, 137),\n",
       " (139, 68),\n",
       " (204, 139),\n",
       " (116, 36),\n",
       " (3, 0),\n",
       " (106, 0),\n",
       " (139, 76),\n",
       " (64, 2),\n",
       " (196, 12),\n",
       " (0, 8),\n",
       " (100, 139),\n",
       " (139, 70),\n",
       " (32, 0),\n",
       " (0, 89),\n",
       " (36, 8),\n",
       " (196, 4),\n",
       " (117, 8),\n",
       " (69, 8),\n",
       " (86, 139),\n",
       " (95, 94),\n",
       " (0, 16),\n",
       " (131, 192),\n",
       " (0, 80),\n",
       " (0, 117),\n",
       " (0, 141),\n",
       " (36, 20),\n",
       " (100, 232),\n",
       " (139, 255),\n",
       " (139, 240),\n",
       " (195, 204),\n",
       " (36, 16),\n",
       " (9, 0),\n",
       " (0, 128),\n",
       " (6, 0),\n",
       " (1, 100),\n",
       " (0, 32),\n",
       " (131, 248)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K1_most_common_Ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a Static Malware Detector/Code/Samples/Benign/lc.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/LogCollector.exe:\n",
      "'DOS Header magic not found.'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/urlproxy.exe:\n",
      "'Invalid NT Headers signature. Probably a NE file'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/evntwin.exe:\n",
      "'DOS Header magic not found.'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/malias.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n",
      "Training a Static Malware Detector/Code/Samples/Malware/VirusShare_7a30183b105b4200fc201925aba4886c.exe:\n",
      "'utf-8' codec can't decode byte 0xb8 in position 0: invalid start byte\n",
      "Training a Static Malware Detector/Code/Samples/Benign/SettingSyncHost.exe:\n",
      "'DOS Header magic not found.'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/InstallUtil.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/Common.DBConnection64.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/oisicon.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/FixSqlRegistryKey_x64.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/ldifde.exe:\n",
      "'DOS Header magic not found.'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/LockAppHost.exe:\n",
      "'DOS Header magic not found.'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/pmsort.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/adaminstall.exe:\n",
      "'DOS Header magic not found.'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/sysprep.exe:\n",
      "'DOS Header magic not found.'\n",
      "Training a Static Malware Detector/Code/Samples/Malware/VirusShare_1a89b7d4fb8ded72e1f8e81ee9352262.exe:\n",
      "'utf-8' codec can't decode byte 0xb1 in position 0: invalid start byte\n",
      "Training a Static Malware Detector/Code/Samples/Benign/RegAsm.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n"
     ]
    }
   ],
   "source": [
    "# Extract N-gram features based on the frequency method\n",
    "# Also, extracts some metadata such as DLL imports, \n",
    "# and PE Sections. We will combine these with\n",
    "# our N-gram features to enrich the sample representation.\n",
    "# This will take a few minutes to run.\n",
    "# Some samples will generate errors such as 'not a PE file',\n",
    "# 'DOS header not found', and 'invalid attribute'. These are OK.\n",
    "importsCorpus_train = []\n",
    "numSections_train = []\n",
    "sectionNames_train = []\n",
    "NgramFeaturesList_train = []\n",
    "y_train = []\n",
    "for i in range(len(samples_train)):\n",
    "    file = samples_train[i]\n",
    "    try:\n",
    "        NGramFeatures = getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list)\n",
    "        pe = pefile.PE(file)\n",
    "        imports = getImports(pe)\n",
    "        nSections = len(pe.sections)\n",
    "        secNames = getSectionNames(pe)\n",
    "        importsCorpus_train.append(imports)\n",
    "        numSections_train.append(nSections)\n",
    "        sectionNames_train.append(secNames)\n",
    "        NgramFeaturesList_train.append(NGramFeatures)\n",
    "        y_train.append(labels_train[i])\n",
    "    except Exception as e: \n",
    "        print(file+\":\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following lines, we define a pipeline of sequential transforms (HashingVectorizer and TfidfTransformer) to extract N-gram featurs and construct feature vectors from the DLL imports and Section names extracted for each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "imports_featurizer = Pipeline([('vect', HashingVectorizer(input='content', ngram_range=(1, 2))),('tfidf', TfidfTransformer(use_idf=True, )),])\n",
    "section_names_featurizer = Pipeline([('vect', HashingVectorizer(input='content', ngram_range=(1, 2))),('tfidf', TfidfTransformer(use_idf=True, )),])\n",
    "importsCorpus_train_transformed = imports_featurizer.fit_transform(importsCorpus_train)\n",
    "sectionNames_train_transformed = section_names_featurizer.fit_transform(sectionNames_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the binary N-gram features with \n",
    "# the DLL imports and section names features to create\n",
    "# vectorized training samples\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "X_train = hstack([NgramFeaturesList_train, importsCorpus_train_transformed,sectionNames_train_transformed, csr_matrix(numSections_train).transpose()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Random Forest classifier\n",
    "# This may take a few minutes.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators=1)\n",
    "clf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training accuracy\n",
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a Static Malware Detector/Code/Samples/Malware/VirusShare_14f3035781bb698c37ad287483af569e.exe:\n",
      "'utf-8' codec can't decode byte 0x8d in position 0: invalid start byte\n",
      "Training a Static Malware Detector/Code/Samples/Benign/fsynonym.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/aspnetca.exe:\n",
      "'DOS Header magic not found.'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/FixSqlRegistryKey_ia64.exe:\n",
      "'PE' object has no attribute 'DIRECTORY_ENTRY_IMPORT'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/BootExpCfg.exe:\n",
      "'DOS Header magic not found.'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/pmgrant.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n",
      "Training a Static Malware Detector/Code/Samples/Benign/newmail.exe:\n",
      "'Invalid e_lfanew value, probably not a PE file'\n"
     ]
    }
   ],
   "source": [
    "# Generate feature vectors for the test samples\n",
    "# This may take a few minutes\n",
    "importsCorpus_test = []\n",
    "numSections_test = []\n",
    "sectionNames_test = []\n",
    "NgramFeaturesList_test = []\n",
    "y_test = []\n",
    "for i in range(len(samples_test)):\n",
    "    file = samples_test[i]\n",
    "    try:\n",
    "        NGramFeatures = getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list)\n",
    "        pe = pefile.PE(file)\n",
    "        imports = getImports(pe)\n",
    "        nSections = len(pe.sections)\n",
    "        secNames = getSectionNames(pe)\n",
    "        importsCorpus_test.append(imports)\n",
    "        numSections_test.append(nSections)\n",
    "        sectionNames_test.append(secNames)\n",
    "        NgramFeaturesList_test.append(NGramFeatures)\n",
    "        y_test.append(labels_test[i])\n",
    "    except Exception as e: \n",
    "        print(file+\":\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "importsCorpus_test_transformed = imports_featurizer.transform(importsCorpus_test)\n",
    "sectionNames_test_transformed = section_names_featurizer.transform(sectionNames_test)\n",
    "X_test = hstack([NgramFeaturesList_test, importsCorpus_test_transformed,sectionNames_test_transformed, csr_matrix(numSections_test).transpose()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9957627118644068"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** The training and test accuracies are unusually high. Can you propose a diagnosis for these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- your response\n",
    "The dataset comprises 435 instances of malware and 300 instances of benign software. The exceptional balance in the dataset could account for the observed high accuracies. Moreover, the selection of the classifier likely contributed significantly. The random forest classifier, known for its robustness as an ensemble model, typically yields favorable outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.3.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For save model for deployment, reuseability,server control save the model and feature as joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['section_names_featurizer.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model and featurizers\n",
    "from joblib import dump\n",
    "\n",
    "# Save model and featurizers\n",
    "dump(clf, 'model.joblib') \n",
    "dump(imports_featurizer, 'imports_featurizer.joblib')\n",
    "dump(section_names_featurizer, 'section_names_featurizer.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaharehArghavani_Building a Static Malware Detector.ipynb\r\n",
      "client1.py\r\n",
      "client.py\r\n",
      "code\r\n",
      "file_b1.exe\r\n",
      "file_b2.exe\r\n",
      "imports_featurizer.joblib\r\n",
      "lost+found\r\n",
      "MalConv-Deploy.ipynb\r\n",
      "model\r\n",
      "model_epoch_15.pt\r\n",
      "model.joblib\r\n",
      "model.tar.gz\r\n",
      "S24_AISec_Client.ipynb\r\n",
      "section_names_featurizer.joblib\r\n",
      "Training a Static Malware Detector\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/\r\n",
      "model/code/\r\n",
      "model/code/inference.py\r\n",
      "model/code/requirements.txt\r\n",
      "model.joblib\r\n"
     ]
    }
   ],
   "source": [
    "!tar -czvf model.tar.gz model model.joblib --exclude '.ipynb_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "no stored variable or alias pt_malconv_model_data3\n"
     ]
    }
   ],
   "source": [
    "# setups\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role, Session\n",
    "\n",
    "sess = Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "%store -r pt_malconv_model_data3\n",
    "\n",
    "try:\n",
    "    pt_malconv_model_data3\n",
    "except NameError:\n",
    "    import json\n",
    "\n",
    "    # copy a pretrained model from a public public to your default bucket\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    \n",
    "    # upload to default bucket\n",
    "    pt_malconv_model_data3 = sess.upload_data(\n",
    "        path=\"model.tar.gz\", bucket=sess.default_bucket(), key_prefix=\"simplified-midterm-03\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-777380185612/simplified-midterm-03/model.tar.gz'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_malconv_model_data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2097253)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate feature vectors for the test sample\n",
    "from joblib import load\n",
    "importsCorpus_test = []\n",
    "numSections_test = []\n",
    "sectionNames_test = []\n",
    "NgramFeaturesList_test = []\n",
    "y_test = []\n",
    "sample_test = ['Training a Static Malware Detector/Code/Samples/Benign/ActivateApplication.exe'] # test sample benign\n",
    "# sample_test = [samples_test[1]] # test sample malware\n",
    "\n",
    "for i in range(len(sample_test)):\n",
    "    file = sample_test[i]\n",
    "    try:\n",
    "        NGramFeatures = getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list)\n",
    "        pe = pefile.PE(file)\n",
    "        imports = getImports(pe)\n",
    "        nSections = len(pe.sections)\n",
    "        secNames = getSectionNames(pe)\n",
    "        importsCorpus_test.append(imports)\n",
    "        numSections_test.append(nSections)\n",
    "        sectionNames_test.append(secNames)\n",
    "        NgramFeaturesList_test.append(NGramFeatures)\n",
    "        y_test.append(labels_test[i])\n",
    "    except Exception as e: \n",
    "        print(file+\":\")\n",
    "        print(e)\n",
    "        \n",
    "\n",
    "imports_featurizer = load('imports_featurizer.joblib')\n",
    "section_names_featurizer = load('section_names_featurizer.joblib')\n",
    "\n",
    "importsCorpus_test_transformed = imports_featurizer.transform(importsCorpus_test)\n",
    "sectionNames_test_transformed = section_names_featurizer.transform(sectionNames_test)\n",
    "X_test_1 = hstack([NgramFeaturesList_test, importsCorpus_test_transformed,sectionNames_test_transformed, csr_matrix(numSections_test).transpose()])\n",
    "X_test_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "sklearn_model = SKLearnModel(model_data=\"s3://sagemaker-us-east-1-777380185612/simplified-midterm-03/model.tar.gz\", \n",
    "                             role=role,\n",
    "                             entry_point=\"inference.py\",\n",
    "                             source_dir=\"model/code/\",\n",
    "                             py_version='py3',\n",
    "                             framework_version=\"1.2-1\",\n",
    "                            dependencies=['model/code/requirements.txt'])\n",
    "\n",
    "predictor = sklearn_model.deploy(instance_type=\"ml.r5.xlarge\", initial_instance_count=1, \n",
    "#                                  \n",
    "                                serializer=JSONSerializer(content_type= 'application/json'),  \n",
    "                                deserializer=JSONDeserializer()\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': [0], 'label': 'benign'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format request\n",
    "import json\n",
    "\n",
    "data = X_test_1.data.tolist()\n",
    "row = X_test_1.row.tolist()\n",
    "col = X_test_1.col.tolist()\n",
    "shape = X_test_1.shape\n",
    "\n",
    "# Create JSON payload\n",
    "payload = {\n",
    "    \"data\": data,\n",
    "    \"row\": row,\n",
    "    \"col\": col,\n",
    "    \"shape\": shape\n",
    "}\n",
    "json_payload = json.dumps(payload)\n",
    "\n",
    "# # set predictor request/response formats\n",
    "predictor.accept = 'application/json'\n",
    "predictor.content_type = 'application/json'\n",
    "\n",
    "result = predictor.predict(json_payload)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-scikit-learn-2024-04-03-23-28-43-349'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction result: {'prediction': [1], 'label': 'malware'}\r\n",
      "True label:  ['Malware']\r\n"
     ]
    }
   ],
   "source": [
    "!python client1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
